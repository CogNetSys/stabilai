# References

A comprehensive list of all scholarly articles, papers, and resources referenced throughout the **Surge-Collapse Training with Entropy Dynamics** project documentation.

---

1. **Adam**:
     - Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*.

2. **Auto-Regressive Neural Networks**:
     - Radford, A., Wu, J., Child, R., et al. (2019). Language models are unsupervised multitask learners. *OpenAI Blog*.

3. **Bayesian Optimization**:
     - Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization of machine learning algorithms. In *Advances in Neural Information Processing Systems* (pp. 2951-2959).

4. **Chizat et al. (2018)**:
     - Chizat, L., Oyallon, E., & Bach, F. (2018). On lazy training in differentiable programming. *Advances in Neural Information Processing Systems*, 2933-2943.

5. **D’Angelo et al. (2023)**:
     - D’Angelo, F., Andriushchenko, M., Varre, A., & Flammarion, N. (2023). Why do we need weight decay in modern deep learning? *arXiv preprint arXiv:2310.04415*.

6. **Elhage et al. (2021)**:
     - Elhage, N., Nanda, N., Olsson, C., et al. (2021). A mathematical framework for transformer circuits. *Transformer Circuits Thread*.

7. **Ji & Telgarsky (2020)**:
     - Ji, Z., & Telgarsky, M. (2020). Gradient descent aligns the layers of deep linear networks. *Advances in Neural Information Processing Systems*, 17176-17186.

8. **Kloberdanz et al. (2022)**:
     - Kloberdanz, E., Kloberdanz, K. G., & Le, W. (2022). Deepstability: A study of unstable numerical methods and their solutions in deep learning. In *Proceedings of the 44th International Conference on Software Engineering* (pp. 586-597).

9. **Liu et al. (2023a)**:
     - Liu, Z., Michaud, E. J., & Tegmark, M. (2023a). Omnigrok: Grokking beyond algorithmic data. *arXiv preprint arXiv:2301.02679*.

10. **Liu et al. (2023b)**:
     - Liu, Z., Zhong, Z., & Tegmark, M. (2023b). Grokking as simplification: A nonlinear complexity perspective.

11. **Power et al. (2022)**:
     - Power, A., Burda, Y., Edwards, H., Babuschkin, I., & Misra, V. (2022). Grokking: Generalization beyond overfitting on small algorithmic datasets. *arXiv preprint arXiv:2201.02177*.

12. **Radford et al. (2019)**:
     - Radford, A., Wu, J., Child, R., et al. (2019). Language models are unsupervised multitask learners. *OpenAI Blog*.

13. **Thilak et al. (2022)**:
     - Thilak, V., Littwin, A., Gershman, S. J., & Pehlevan, C. (2022). The slingshot mechanism: An empirical study of adaptive optimizers and the grokking phenomenon.

14. **Varma et al. (2023)**:
     - Varma, A., Shah, R., Kenton, Z., Kram´ar, J., & Kumar, R. (2023). Explaining grokking through circuit efficiency. *arXiv preprint arXiv:2309.02390*.

15. **Zunković & Ilievski (2024)**:
     - Zunković, B., & Ilievski, E. (2024). Grokking phase transitions in learning local rules with gradient descent. *Journal of Machine Learning Research*, 25(199), 1-52.

---

For more detailed information and access to the full papers, please refer to the provided links or search for the papers using their titles and authors.

---
